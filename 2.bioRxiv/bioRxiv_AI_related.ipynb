{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.ssl_ import create_urllib3_context\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve SLLerror and requests error(caused by prox)\n",
    "CIPHERS = (\n",
    "    'ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+HIGH:'\n",
    "    'DH+HIGH:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+HIGH:RSA+3DES:!aNULL:'\n",
    "    '!eNULL:!MD5'\n",
    ")\n",
    "\n",
    "class DESAdapter(HTTPAdapter):\n",
    "    \"\"\"\n",
    "    A TransportAdapter that re-enables 3DES support in Requests.\n",
    "    \"\"\"\n",
    "    def init_poolmanager(self, *args, **kwargs):\n",
    "        context = create_urllib3_context(ciphers=CIPHERS)\n",
    "        kwargs['ssl_context'] = context\n",
    "        return super(DESAdapter, self).init_poolmanager(*args, **kwargs)\n",
    "\n",
    "    def proxy_manager_for(self, *args, **kwargs):\n",
    "        context = create_urllib3_context(ciphers=CIPHERS)\n",
    "        kwargs['ssl_context'] = context\n",
    "        return super(DESAdapter, self).proxy_manager_for(*args, **kwargs)\n",
    "\n",
    "s = requests.Session()\n",
    "s.mount('https://api.rxivist.org', DESAdapter())\n",
    "headers = {'User-Agent': 'User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n",
    "# r = s.get('https://some-3des-only-host.com/some-path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grepKeywords(key):\n",
    "    # build a blank df\n",
    "    outDf = pd.DataFrame(columns=[\"idx\",\"downloads\",\"category\",\"post_date\",\"title\"])\n",
    "    # the api of bioRxiv\n",
    "    url = \"https://api.rxivist.org/v1/papers?q=\"+key+\"&metric=downloads\"\n",
    "    json_input = s.get(url , headers = headers)\n",
    "    json_result = json.loads(json_input.text)\n",
    "    pagesNum = json_result[\"query\"][\"final_page\"]\n",
    "    totalNum = json_result[\"query\"][\"total_results\"]\n",
    "    \n",
    "    for i in range(pagesNum+1):\n",
    "        url_tmp = \"https://api.rxivist.org/v1/papers?q=\"+key+\"&metric=downloads&page=\"+str(i)\n",
    "        json_input_tmp = s.get(url_tmp , headers = headers)\n",
    "        json_result_tmp = json.loads(json_input_tmp.text)\n",
    "        for n in range(20):\n",
    "            try:\n",
    "                tmpT = json_result_tmp[\"results\"][n]\n",
    "                outDf = outDf.append(pd.DataFrame(\n",
    "                    {\"idx\":tmpT[\"id\"] , \"downloads\":tmpT[\"metric\"] ,\n",
    "                     \"category\":tmpT[\"category\"] , \"post_date\":tmpT[\"first_posted\"],\n",
    "                     \"title\":tmpT[\"title\"]},index=[0]), ignore_index=True)\n",
    "            except IndexError:\n",
    "                break\n",
    "        \n",
    "        time.sleep(3)\n",
    "    \n",
    "    if totalNum == outDf.shape[0]:\n",
    "        return(totalNum, outDf)   \n",
    "    else:\n",
    "        return(\"Some errors in \"+key+\" searching.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial intelligence', 'machine learning', 'deep learning', 'unsupervised learning', 'supervised learning', 'artificial neuron', ' transfer learning', 'complex network', 'multitask learning', 'metric learning', 'ensemble learning', 'convolutional neural network', 'recurrent neural network', 'autoencoder', 'logistic regression', 'support vector machine', 'k nearest neighbors algorithm', 'random forest', 'generative adversarial network', 'bayesian network', 'decision tree', 'natural language processing', 'pattern recognition', 'image search', 'fuzzy logic', 'human computer interaction', 'life prediction', 'feature extraction', 'intelligent search', 'machine vision', 'fuzzy control', 'expert system', 'image recognition', 'neuromotor computing', 'knowledge reasoning', 'genetic algorithm', 'speech recognition', 'perceptron', 'wavelet transform', 'face recognition', 'intelligent robot']\n"
     ]
    }
   ],
   "source": [
    "# Final word list\n",
    "wordStorage = \"artificial-intelligence OR machine-learning OR deep-learning OR unsupervised-learning OR supervised-learning OR artificial-neuron OR  transfer-learning OR complex-network OR multitask-learning OR metric-learning OR ensemble-learning OR convolutional-neural-network OR recurrent-neural-network OR autoencoder OR logistic-regression OR support-vector-machine OR k-nearest-neighbors-algorithm OR random-forest OR generative-adversarial-network OR bayesian-network OR decision-tree OR natural-language-processing OR pattern-recognition OR image-search OR fuzzy-logic OR human-computer-interaction OR life-prediction OR feature-extraction OR intelligent-search OR machine-vision OR fuzzy-control OR expert-system OR image-recognition OR neuromotor-computing OR knowledge-reasoning OR genetic-algorithm OR speech-recognition OR perceptron OR wavelet-transform OR face-recognition OR intelligent-robot\".lower().replace(\"-\",\" \").split(\" or \")\n",
    "### Neural network must be removed.\n",
    "print(wordStorage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outDf = pd.DataFrame({\"idx\":\"idx\",\"downloads\":\"downloads\",\"category\":\"category\",\n",
    "#                       \"post_date\":\"post_date\",\"title\":\"title\"},index=[0])\n",
    "outDf = pd.DataFrame(columns=[\"idx\",\"downloads\",\"category\",\"post_date\",\"title\"])\n",
    "count = 0\n",
    "for word in wordStorage:\n",
    "    tmpDf = grepKeywords(word)\n",
    "    count += tmpDf[0]\n",
    "    outDf = pd.concat([outDf , tmpDf[1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of papers: 5033\n",
      "Duplicate processing:\n"
     ]
    }
   ],
   "source": [
    "print(\"total number of papers: \"+str(count))\n",
    "# outDf = outDf.drop(0)\n",
    "if count == outDf.shape[0]:\n",
    "    print(\"Duplicate processing:\")\n",
    "    outDf1 = outDf.drop_duplicates([\"idx\"])\n",
    "    outDf2 = outDf1.reset_index(drop=True) \n",
    "    outDf2.to_csv(\"./bioRxiv_AIrelated_papers_Final.tsv\" , sep=\"\\t\" , index=0)\n",
    "else:\n",
    "    print(\"Some errors here, but the origin matrix is output.\")\n",
    "    outDf.to_csv(\"./bioRxiv_onlyConcat_AIrelated_papers.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
